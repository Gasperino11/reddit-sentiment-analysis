{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on titles of Reddit Posts using Google Cloud NLP\n",
    "\n",
    "The below code makes use of Reddit RSS feeds in order to extract some basic information (such as title and author) about posts from a list of subreddits provided by the user. After the information is extarcted, a function is then called to  generate the sentiment analysis on the title of the post. A function is also provided to convert the JSON data into CSVs to make it easier to load and use for BI purposes.\n",
    "\n",
    "This project was completed as my final project for a cloud computing course as a part of getting my master's at the University of Missouri. The intent of the project was to showcase, at a small scale, the ability to extract data via any method (such as web scrapping), utilize what we learned about cloud computing in order to run machine learning of some kind on it, and then make the insights of the machine learning available to a business intelligence tool. The picture somes up the intended data flow.\n",
    "\n",
    "![Specific_Project_1.png MISSING](../images/Specific_Project_1.png)\n",
    "\n",
    "I chose to do my project on extracting data from reddit using RSS feeds, analyzing the sentiment of titles, and exposing that data through an R Shiny dashboard. The below code is exclusive to extracting the data, running Google's NLP API on it, and then flattening it into a CSV. The Shiny app can be found at [insert app link here]() and code for it can be found in the shinyApp folder.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Dependancies](#Dependancies)\n",
    "- [Stackoverflow Functions](#stackoverflow)\n",
    "- [Reddit to JSON](#json)\n",
    "- [Sentiment Analysis](#sentiment)\n",
    "- [Flattening the Output](#flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependancies <a name=\"Dependancies\"></a>\n",
    "\n",
    "The below functions are built out using a few common Python libraries so those need to be installed and loaded in order for all this to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dependancies\n",
    "import json\n",
    "import feedparser\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import language_v1\n",
    "from google.cloud.language_v1 import enums\n",
    "from google.cloud.language_v1 import types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stackoverflow Functions <a name=\"stackoverflow\"></a>\n",
    "\n",
    "I make use for Beautiful Soup in order to parse the RSS feeds and get the data I need. Stackoverflow was a big help here, as indicated by the comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First two functions from: https://stackoverflow.com/questions/1936466/beautifulsoup-grab-visible-webpage-text\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit to JSON <a name='json'></a>\n",
    "\n",
    "These functions are put together in the same cell as find_author() is necessary for reddit_to_json to work. I noticed that with the above functions, when an author was multiple users or not coming through in the RSS feed, it would break reddit_to_json. Rather than remove the author, I thought it would be best to just make it null so that way users could see authors who post a lot. It turns out this wasn't particularly helpful since by doing this a majority of authors ended up null (something I wasn't expecting). I'm not sure if I'll ever return to this project, but I think if I do that will be one of the things I try to make better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to find the author of a reddit post's author and if there isn't one to return null\n",
    "def find_author(reddit_post):\n",
    "    try:\n",
    "        author = reddit_post[\"author\"]\n",
    "    except:\n",
    "        author = None\n",
    "\n",
    "    return author\n",
    "\n",
    "#a function to parse reddit RSS feeds and turn them into json files\n",
    "def reddit_to_json(subreddits,sort,posts_from,amt):\n",
    "    \"\"\" A function to convert a reddit rss feed into a json file.\n",
    "    \n",
    "    This function takes four parameters:\n",
    "        - subreddits is a list of subreddits that you want to parse.\n",
    "        - sort is the type of sorting you apply to the subreddit in the rss feed URL.\n",
    "          It can take any parameter and rss feed url can take. Some options include \"top\", \"hot\", or \"new\".\n",
    "        - posts_from specifies the timeframe for which posts are extracted when sorting by \"top\". By default\n",
    "          this is set to \"all\" to extract the top posts from all time. Options are \"hour\",\"day\",\"week\", \"month\",\n",
    "          \"year\", and \"all\".\n",
    "        - amt is the amount of posts you want to fetch which is equal to the number of posts a subreddit can show.\n",
    "          By default, this is set to 25 but can be changed to any integer between 1 and 100.\n",
    "    \n",
    "    It will return a JSON of 25 reddit posts based off your sorting and posts_from paratemter (for example,\n",
    "    when sort is \"top\" and posts_from is \"all\" it will grab you the top 25 posts of all time). It will return the\n",
    "    subreddit the post is in, the title, the author, the date posted, a link to the post, and the summary text.\n",
    "    \"\"\"\n",
    "    \n",
    "    #create for loop that will parse through rss feeds for all subreddits in our subreddit list\n",
    "    for subreddit in subreddits:\n",
    "        #create empty dict with empty list of posts to append to for printing purposes\n",
    "        reddit = {}\n",
    "        reddit[\"posts\"] = []\n",
    "        \n",
    "        #create rss url based off sort and posts_from parameters\n",
    "        if (sort == \"top\"):\n",
    "            if (posts_from is None):\n",
    "                rss_url = 'http://www.reddit.com/r/%s/top/.rss?limit=%s&sort=%s&t=all' % (subreddit,amt,sort)\n",
    "                filename = '%s_%s_all.json' % (subreddit, sort)\n",
    "            else:\n",
    "                rss_url = 'http://www.reddit.com/r/%s/top/.rss?limit=%s&sort=%s&t=%s' % (subreddit,amt,sort,posts_from)\n",
    "                filename = '%s_%s_%s.json' % (subreddit,sort,posts_from)\n",
    "        else: \n",
    "            rss_url = 'http://www.reddit.com/r/%s/.rss?limit=%s&sort=%s' % (subreddit,amt,sort)\n",
    "            filename = '%s_%s.json' % (subreddit,sort)\n",
    "            \n",
    "        feed = feedparser.parse(rss_url)\n",
    "\n",
    "        #create conditional to show errors\n",
    "        if (feed['bozo'] == 1):\n",
    "            print(\"Error Reading/Parsing Feed XML Data for /r/{}\".format(subreddit))    \n",
    "        else:\n",
    "            print(\"Making json of %s posts from /r/%s...\" % (sort,subreddit))\n",
    "            for item in feed[\"items\"]:\n",
    "                reddit[\"posts\"].append({\n",
    "                    \"subreddit\":subreddit,\n",
    "                    \"title\":item[\"title\"],\n",
    "                    \"author\":find_author(item),\n",
    "                    \"link\":item[\"link\"],\n",
    "                    \"datetime\":item[\"date\"],\n",
    "                    \"summary_text\": text_from_html(item[\"summary\"])\n",
    "                })\n",
    "                \n",
    "        #if directory does not exist, make it\n",
    "        if not os.path.exists('json_outputs/'):\n",
    "            os.makedirs('json_outputs/')\n",
    "\n",
    "        #output the data as a json file\n",
    "        with open(filename, 'w') as outfile:\n",
    "            json.dump(reddit, outfile)\n",
    "\n",
    "        #move files to output folder\n",
    "        os.rename(filename, 'json_outputs/%s' % filename)\n",
    "    \n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis <a name='sentiment'></a>\n",
    "\n",
    "The below function will call the Google Cloud APIs (using a user's Google Cloud credentials) and run sentiment analysis. It will then update the JSONs and merge them together into one JSON file that could then be uploaded into a NoSQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #a function to add sentiment analysis data and compile all reddit jsons from 'reddit_to_json' function\n",
    "def reddit_sentiment_analysis(credentials_path,data_path='json_outputs/',results_path='results/'):\n",
    "    \"\"\" A function that takes reddit json data (created by the reddit_to_json function) \n",
    "    and adds sentiment and entity analysis data for the title using the Google Cloud NLP API.\n",
    "    \n",
    "    This function takes three parameters:\n",
    "    - The credentials_path specifies the path to a json file used for Google Cloud credentials. Without a credentials\n",
    "      file provided, an exception will be rasied.\n",
    "    - The data_path specifies the path where the json inputs can be found. The 'reddit_to_json' function will \n",
    "      create a directory called 'json_outputs' and so the default value for this parameter is set to 'json_outputs'.\n",
    "      If the output directory of 'reddit_to_json' was changed or the files moved, you will need to set this parameter.\n",
    "    - The results_path specifies the path specifies where the final json will land. By default\n",
    "      a folder called 'results' will be created and the file will be placed there.\n",
    "    \n",
    "    It will return a single json with all of the jsons in the file path appended together and sentiment analysis added\n",
    "    for each post. It will print it to result/reddit_sentiment_analysis.json\n",
    "    \"\"\"\n",
    "        \n",
    "    #check for credentials file and if it doesn't exist, inform the user\n",
    "    if credentials_path is None:\n",
    "        raise Exception(\"credentials_path: No Google Cloud credentials found - please provide a JSON file with credentials.\")\n",
    "\n",
    "    #establish empty list of files\n",
    "    print('Reading JSON from %s' % (data_path))\n",
    "    files = []\n",
    "\n",
    "    #read in files from path so we can loop through it later\n",
    "    for file in os.listdir(data_path):\n",
    "        if file.endswith(\".json\"):\n",
    "            files.append(file)\n",
    "\n",
    "    #let the user know how many files were found so they can validate the path is correct\n",
    "    #and let them know if no files were found\n",
    "    if len(files) == 0:\n",
    "        print(\"Error: No Files Found!\")\n",
    "    print('Found %s files in provided path' % (len(files)))\n",
    "\n",
    "    #set up credentials for Google Cloud and set up NLP client API\n",
    "    print(\"Fetching Google Cloud Credentials from {} and establishing connection to API\".format(credentials_path))\n",
    "    credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "    client = language_v1.LanguageServiceClient(credentials=credentials)\n",
    "    type_ = enums.Document.Type.PLAIN_TEXT\n",
    "    encoding_type = enums.EncodingType.UTF8\n",
    "\n",
    "    #create empty outputlist that will ultimately be export to JSON\n",
    "    outputlist = []\n",
    "\n",
    "    #loop through files to do sentiment analysis on\n",
    "    for file in files:\n",
    "        print(\"Reading {}...Analyzing Sentiment and Entities\".format(data_path+file))\n",
    "        openfile = open(data_path+file)\n",
    "        data = json.load(openfile)\n",
    "\n",
    "        #analyze title sentiment and entities\n",
    "        for post in data['posts']:\n",
    "            #build a document with the post title as content\n",
    "            document = types.Document(content=post['title'],type=enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "            #update post with title sentiment score & magnitude and number of entities\n",
    "            post.update({\n",
    "                'sentiment_score':client.analyze_sentiment(document, encoding_type=encoding_type).document_sentiment.score,\n",
    "                'sentiment_magnitude':client.analyze_sentiment(document, encoding_type=encoding_type).document_sentiment.magnitude,\n",
    "                'num_entities':len(client.analyze_entities(document, encoding_type=encoding_type).entities)\n",
    "            })\n",
    "\n",
    "        #append updated data to empty list\n",
    "        outputlist.append(data['posts'])\n",
    "\n",
    "        #Google's API has a limit of 60 calls per minute per user\n",
    "        #so we sleep for 60 seconds here to avoid timeouts\n",
    "        print(\"Done with {}...Waiting for 60 Seconds to avoid API timeout\".format(file))\n",
    "        time.sleep(60)\n",
    "\n",
    "    #print results to json\n",
    "    print(\"Done! Printing file to results folder.\")\n",
    "    \n",
    "    #assign file name\n",
    "    file_name = 'reddit_sentiment_analysis.json'\n",
    "    \n",
    "    #make the results directory if it doesn't exist\n",
    "    if not os.path.exists(results_path):\n",
    "        os.makedirs(results_path)\n",
    "    \n",
    "    #dump outputlist as json file into working directory\n",
    "    with open(file_name,\"w\") as outfile:\n",
    "        json.dump(outputlist,outfile)\n",
    "        \n",
    "    #move file\n",
    "    os.rename(file_name,results_path+file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening the Output <a name='flatten'></a>\n",
    "\n",
    "The below output will take the JSON we generated from the above function and turn it into a CSV. This is good for low volumes of data (i.e. not extracting many subreddits or posts) and makes analysis in a BI tool quicker. I think a lot more people are comfortable with the data structure of a CSV rather than a JSON as well (at least in my experiernce) so it enables more users to take advantage of the insights generated by the sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to take the output from 'sentiment_analysis_reddit'\n",
    "def flatten_sentiment_analysis(input_file='results/reddit_sentiment_analysis.json',output_path='results/flattened_sentiment_analysis.csv'):\n",
    "    \"\"\" A function that takes the json output from the 'sentiment_analysis_reddit' function and flattens it to CSV\n",
    "    \n",
    "    This function can take two parameters:\n",
    "    - The input_file is the filepath for the json file. It is by default results/reddit_sentiment_analysis.json\n",
    "      which is the default output of the 'sentiment_analysis_reddit' function\n",
    "    - The output_file is the desired landing place for the CSV. By default, this is set to 'results/flattened_sentiment_analysis.csv'\n",
    "    \n",
    "    Currently this function CANNOT be run without first running the 'sentiment_analysis_reddit' function in order\n",
    "    to create one unified file. A way to flatten reddit json output from 'reddit_to_json' will be coming in the future\n",
    "    \"\"\"\n",
    "    \n",
    "    #open file\n",
    "    openfile = open(input_file)\n",
    "    data = json.load(openfile)\n",
    "\n",
    "    #build empty dataframe\n",
    "    column_names = ['post_id','subreddit','author','datetime','title','sentiment_score','sentiment_magnitude','num_entities']\n",
    "    df = pd.DataFrame(columns = column_names)\n",
    "\n",
    "    #set subreddit index and post_index\n",
    "    subreddit_index = 1000\n",
    "    post_index = 1\n",
    "\n",
    "    #loop through the json building out the dataframe\n",
    "    print(\"Flattening json to csv...\")\n",
    "    for i in range(0,len(data)):\n",
    "        for j in range(0,len(data[i])):\n",
    "            post_id = subreddit_index + post_index\n",
    "            post_df = pd.DataFrame(\n",
    "                [[post_id,\n",
    "                  data[i][j]['subreddit'],\n",
    "                  data[i][j]['author'],\n",
    "                  data[i][j]['datetime'],\n",
    "                  data[i][j]['title'],\n",
    "                  data[i][j]['sentiment_score'],\n",
    "                  data[i][j]['sentiment_magnitude'],\n",
    "                  data[i][j]['num_entities']\n",
    "                 ]],\n",
    "                columns=column_names\n",
    "            )\n",
    "            #increment\n",
    "            post_index = post_index + 1\n",
    "\n",
    "            #append post_df to empty df\n",
    "            df = df.append(post_df)\n",
    "\n",
    "        #increment subreddit index and de-increment post_index\n",
    "        post_index = 1\n",
    "        subreddit_index = subreddit_index + 1000\n",
    "\n",
    "    #print results to CSV\n",
    "    print(\"Printing result to {}...\".format(output_path))\n",
    "    df.to_csv(output_path,header=True,index=False,encoding='utf-8')\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Data <a name='extraction'></a>\n",
    "\n",
    "Finally getting to the part that matters! I ran my functions with the following list of subreddits, which at the time this was done (April 25th, 2020) were the top 25 subreddits by subscriber count (minus /r/Announcements). I wanted to see if we could get any idea of a sentiment of titles for these subreddits and there were definitely some interesting things going on.\n",
    "\n",
    "The cell will extract the data into JSONs for each of the subreddits, do the sentiment analysis, and then merge the data into one JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfortunately there is no way to easily extract a list of top subreddits from  reddit itself\n",
    "# so I pulled them from http://redditlist.com/ as shown in the screenshot above\n",
    "# I removed announcements as it does not have user generated content\n",
    "subreddits = [\"funny\",\"AskReddit\",\"gaming\",\"pics\",\"aww\",\"science\",\"worldnews\",\"Music\",\"movies\",\"videos\",\n",
    "              \"todayilearned\",\"news\",\"IAmA\",\"gifs\",\"Showerthoughts\",\"EarthPorn\",\"askscience\",\"Jokes\",\"food\",\n",
    "              \"explainlikeimfive\",\"books\",\"LifeProTips\",\"Art\",\"mildlyinteresting\",\"DIY\"]\n",
    "\n",
    "reddit_to_json(subreddits,\"top\",\"all\",100)\n",
    "reddit_sentiment_analysis('mu-dsa-course-8635-sp20-ffc2322a17d7.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this will then flatten it and create a CSV. Never hurts to flatten things if you can, in my opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_sentiment_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
